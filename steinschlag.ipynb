{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "from scipy.stats import  expon, norm, gamma, beta, lognorm\n",
    "from scipy.stats._continuous_distns import beta_gen, gamma_gen\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1 = pd.read_csv('data/out_1.csv', usecols=[\"Datum\", \"Uhrzeit\", \"Masse [kg]\", \"Geschwindigkeit [m/s]\"])\n",
    "zone2 = pd.read_csv('data/out_2.csv', usecols=[\"Date\", \"Uhrzeit\", \"m [kg]\", \"v [m/s]\"])\n",
    "zone1.columns = [\"date\", \"time\", \"kg\", \"m/s\"]\n",
    "zone2.columns = [\"date\", \"time\", \"kg\", \"m/s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([zone1, zone2], axis=1, keys=['zone1', 'zone2']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "print(zone1.isna().sum())\n",
    "\n",
    "# Check for zeros\n",
    "print(zone1.eq(0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop lines with only NA values\n",
    "zone1 = zone1.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "print(zone2.isna().sum())\n",
    "\n",
    "# Check for zeros\n",
    "print(zone2.eq(0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop lines with only NA values\n",
    "zone2 = zone2.dropna(how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the value to the median in the row where 'kg' equals 0.0\n",
    "# This rock is not removed from the data, because we do not have a lot of data and this might just have been an error in the measurement\n",
    "zone2.loc[zone2['kg'] == 0.0, 'kg'] = zone2['kg'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([zone1, zone2], axis=1, keys=['zone1', 'zone2']).describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1.plot(x='kg', y='m/s', kind='scatter', c='red', label='zone1', title='Mass vs Velocity Zone 1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone2.plot(x='kg', y='m/s', kind='scatter', c='blue', label='zone2', title='Mass vs Velocity Zone 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(zone1['kg'], zone1['m/s'], c='red', label='zone1')\n",
    "ax.scatter(zone2['kg'], zone2['m/s'], c='blue', label='zone2')\n",
    "ax.legend()\n",
    "ax.axes.set_xlabel('m [kg]')\n",
    "ax.axes.set_ylabel('v [m/s]')\n",
    "plt.title('Mass vs Velocity in both Zones')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zonen 1 und 2 sollten nicht gemischt werden, weil sonst sehr schwere steine generiert werden k√∂nnten die eine hohe Geschwindigkeit haben, was in den Aufzeichnungen aber nicht vorkomt (nicht gleiche Grundgesammtheit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1['kj'] = 0.5 * zone1['kg'] * (zone1['m/s']**2) /1000\n",
    "zone2['kj'] = 0.5 * zone2['kg'] * (zone2['m/s']**2) /1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' and 'time' columns to datetime for zone1\n",
    "zone1['datetime'] = pd.to_datetime(zone1['date'] + ' ' + zone1['time'])\n",
    "\n",
    "# Calculate the time difference between each row and the row above, in hours then set the first row to the median of all rows\n",
    "zone1['timediv h'] = (zone1['datetime'] - zone1['datetime'].shift()).fillna(pd.Timedelta(seconds=0))\n",
    "zone1['timediv h'] = zone1['timediv h'].apply(lambda x: int(round(x.total_seconds() / 3600)))\n",
    "\n",
    "# Set the first row to the median of all rows so we don't loose a value\n",
    "zone1.loc[0, 'timediv h'] = zone1['timediv h'].median()\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(zone1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' and 'time' columns to datetime for zone2\n",
    "zone2['datetime'] = pd.to_datetime(zone2['date'] + ' ' + zone2['time'])\n",
    "\n",
    "# Calculate the time difference between each row and the row above, in hours\n",
    "zone2['timediv h'] = (zone2['datetime'] - zone2['datetime'].shift()).fillna(pd.Timedelta(seconds=0))\n",
    "zone2['timediv h'] = zone2['timediv h'].apply(lambda x: int(round(x.total_seconds() / 3600)))\n",
    "\n",
    "# Set the first row to the median of all rows so we don't loose a value\n",
    "zone2.loc[0, 'timediv h'] = zone2['timediv h'].median()\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(zone2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 10\n",
    "\n",
    "# Plot the histograms for 'kg'\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.hist(zone1['kg'], bins=num_bins, color='red', label='Zone 1')\n",
    "ax2.hist(zone2['kg'], bins=num_bins, color='blue', label='Zone 2')\n",
    "\n",
    "ax1.set_xlabel('kg')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.legend()\n",
    "ax2.set_xlabel('kg')\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the histograms for 'kj'\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.hist(zone1['kj'], bins=num_bins, color='red', label='Zone 1')\n",
    "ax2.hist(zone2['kj'], bins=num_bins, color='blue', label='Zone 2')\n",
    "\n",
    "ax1.set_xlabel('kj')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.legend()\n",
    "ax2.set_xlabel('kj')\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the histograms for 'm/s'\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.hist(zone1['m/s'], bins=num_bins, color='red', label='Zone 1')\n",
    "ax2.hist(zone2['m/s'], bins=num_bins, color='blue', label='Zone 2')\n",
    "\n",
    "ax1.set_xlabel('m/s')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.legend()\n",
    "ax2.set_xlabel('m/s')\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the histograms for 'timediv h'\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.hist(zone1['timediv h'], bins=num_bins, color='red', label='Zone 1')\n",
    "ax2.hist(zone2['timediv h'], bins=num_bins, color='blue', label='Zone 2')\n",
    "\n",
    "ax1.set_xlabel('timediv h')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.legend()\n",
    "ax2.set_xlabel('timediv h')\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([zone1, zone2], axis=1, keys=['zone1', 'zone2']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdf_fit(data):\n",
    "    distributions = [ss.norm, ss.lognorm, ss.expon, ss.gamma]\n",
    "\n",
    "    # Plot the CDF of the data and the fitted distributions\n",
    "    plt.hist(data, bins=len(data), density=True, cumulative=True, alpha=0.5, label='Data')\n",
    "    x = np.linspace(data.min(), data.max()*1.2, 100)\n",
    "    \n",
    "    for dist in distributions:\n",
    "        params = dist.fit(data)\n",
    "        ll = -dist.logpdf(data, *params).sum().round(0)\n",
    "        plt.plot(x, dist(*params).cdf(x), label=f'{dist.name}, score: {ll}')\n",
    "        plt.xlabel(data.name)\n",
    "        plt.ylabel('Cumulative probability')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_fit(zone1['kg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_fit(zone2['kg']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_fit(zone1['m/s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_fit(zone2['m/s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_fit(zone1['timediv h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_fit(zone1['timediv h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to simulate a dataframe for the next number of years it will estimate the number of events it takes and generate a frame \n",
    "def simulate_zone(zone_df, num_years=200):\n",
    "    \n",
    "    timediv_mean = zone_df['timediv h'].mean()\n",
    "    \n",
    "    # Calculate  the deviation\n",
    "    timediv_params = expon.fit(zone_df['timediv h'])\n",
    "    kg_params = gamma.fit(zone_df['kg'])\n",
    "    v_params = norm.fit(zone_df['m/s'])\n",
    "    # Calculate number of observations for given number of years\n",
    "    total_hours = num_years * 365.25 * 24\n",
    "    num_observations = int(total_hours / timediv_mean)\n",
    "    \n",
    "    # Set the starting datetime to January 1st, 2000, 00:00:00\n",
    "    current_datetime = datetime(2000, 1, 1, 0, 0, 0)\n",
    "\n",
    "    # Initialize the new dataframe and generate the data\n",
    "    simulated_df = pd.DataFrame(index=range(num_observations))\n",
    "    simulated_df['timediv h'] = expon(*timediv_params).rvs(size=num_observations).round(0)\n",
    "    simulated_df['datetime'] = simulated_df['timediv h'].cumsum().apply(lambda x: current_datetime + timedelta(hours=x))\n",
    "    simulated_df['kg'] = gamma(*kg_params).rvs(size=num_observations).round(0)\n",
    "    simulated_df['m/s'] = norm(*v_params).rvs(size=num_observations).round(1)\n",
    "    simulated_df['kj'] = 0.5 * simulated_df['kg'] * (simulated_df['m/s']**2) /1000\n",
    "    \n",
    "    return simulated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulated zones\n",
    "simulated1 = simulate_zone(zone1)\n",
    "simulated1['zone'] = 1\n",
    "simulated2 = simulate_zone(zone2)\n",
    "simulated2['zone'] = 2\n",
    "simulated1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([zone1, simulated1], axis=1, keys=['zone1', 'simulated1']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables\n",
    "num_bins = 200\n",
    "cumulative = True\n",
    "\n",
    "# Plot the histograms for 'kg', 'm/s', and 'timediv h'\n",
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(10, 10))\n",
    "\n",
    "# Histogram for 'kg'\n",
    "ax1.hist(zone1['kg'], bins=num_bins, color='red', alpha=0.5, label='Zone 1', density=True, cumulative=cumulative)\n",
    "ax1.hist(simulated1['kg'], bins=num_bins, color='orange', alpha=0.5, label='Simulated 1', density=True, cumulative=cumulative)\n",
    "ax2.hist(zone2['kg'], bins=num_bins, color='blue', alpha=0.5, label='Zone 2', density=True, cumulative=cumulative)\n",
    "ax2.hist(simulated2['kg'], bins=num_bins, color='green', alpha=0.5, label='Simulated 2', density=True, cumulative=cumulative)\n",
    "\n",
    "ax1.set_xlabel('kg')\n",
    "ax1.set_ylabel('Frequency Density')\n",
    "ax1.legend()\n",
    "ax2.set_xlabel('kg')\n",
    "ax2.legend()\n",
    "\n",
    "# Histogram for 'm/s'\n",
    "ax3.hist(zone1['m/s'], bins=num_bins, color='red', alpha=0.5, label='Zone 1', density=True, cumulative=cumulative)\n",
    "ax3.hist(simulated1['m/s'], bins=num_bins, color='orange', alpha=0.5, label='Simulated 1', density=True, cumulative=cumulative)\n",
    "ax4.hist(zone2['m/s'], bins=num_bins, color='blue', alpha=0.5, label='Zone 2', density=True, cumulative=cumulative)\n",
    "ax4.hist(simulated2['m/s'], bins=num_bins, color='green', alpha=0.5, label='Simulated 2', density=True, cumulative=cumulative)\n",
    "\n",
    "ax3.set_xlabel('m/s')\n",
    "ax3.set_ylabel('Frequency Density')\n",
    "ax3.legend()\n",
    "ax4.set_xlabel('m/s')\n",
    "ax4.legend()\n",
    "\n",
    "# Histogram for 'timediv h'\n",
    "ax5.hist(zone1['timediv h'], bins=num_bins, color='red', alpha=0.5, label='Zone 1', density=True, cumulative=cumulative)\n",
    "ax5.hist(simulated1['timediv h'], bins=num_bins, color='orange', alpha=0.5, label='Simulated 1', density=True, cumulative=cumulative)\n",
    "ax6.hist(zone2['timediv h'], bins=num_bins, color='blue', alpha=0.5, label='Zone 2', density=True, cumulative=cumulative)\n",
    "ax6.hist(simulated2['timediv h'], bins=num_bins, color='green', alpha=0.5, label='Simulated 2', density=True, cumulative=cumulative)\n",
    "\n",
    "ax5.set_xlabel('timediv h')\n",
    "ax5.set_ylabel('Frequency Density')\n",
    "ax5.legend()\n",
    "ax6.set_xlabel('timediv h')\n",
    "ax6.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_simulated_to_original(original_column, simulated_column, num_bins = 200, cumulative = True):\n",
    "\n",
    "    plt.hist(original_column, bins=num_bins, color='red', alpha=0.5, label='Zone 1', density=True, cumulative=cumulative)\n",
    "    plt.hist(simulated_column, bins=num_bins, color='orange', alpha=0.5, label='Simulated 1', density=True, cumulative=cumulative)\n",
    "\n",
    "    ax1.set_xlabel(original_column.name)\n",
    "    ax1.set_ylabel('Frequency Density')\n",
    "    ax1.legend()\n",
    "\n",
    "compare_simulated_to_original(zone1['kg'], simulated1['kg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Determine the latest end datetime of the two dataframes\n",
    "max_datetime = min(simulated1['datetime'].max(), simulated2['datetime'].max())\n",
    "\n",
    "# Set the end datetime of both dataframes to be the same\n",
    "simulated1 = simulated1[simulated1['datetime'] <= max_datetime]\n",
    "simulated2 = simulated2[simulated2['datetime'] <= max_datetime]\n",
    "\n",
    "# Merge the two dataframes together, sort by datetime, and reset the index\n",
    "simulated_df = pd.concat([simulated1, simulated2])\n",
    "simulated_df = simulated_df.sort_values('datetime')\n",
    "simulated_df = simulated_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_df.tail(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because the reaction time is 24h we will asume that the nets will get emptied every evening if there are stones in it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column that calculates the cumulative kg already in the net.\n",
    "\n",
    "# first group the data by date\n",
    "grouped_df = simulated_df.groupby(simulated_df['datetime'].dt.date)\n",
    "\n",
    "# then calculate the cumulative sum of 'kg' within each group \n",
    "simulated_df['cumulative_kg'] = grouped_df['kg'].cumsum()\n",
    "# and subtract the 'kg' valueof the new stone to get the weight in the net\n",
    "simulated_df['cumulative_kg'] =  simulated_df['cumulative_kg'] - simulated_df['kg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should we disregard the rest stones of the day if the net broke trough?\n",
    "# after this the road probably gets closed\n",
    "\n",
    "# Add a new column 'breakthrough'\n",
    "simulated_df['breakthrough'] = 0\n",
    "\n",
    "# Set breakthrough to 1 where conditions are met\n",
    "condition1 = (simulated_df['kj'] > 1000)\n",
    "condition2 = (simulated_df['cumulative_kg'] > 2000) & (simulated_df['kj'] > 500)\n",
    "simulated_df.loc[condition1 | condition2, 'breakthrough'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i look at the tail to make shure the cumulative_kg got calculated correctly.\n",
    "\n",
    "simulated_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability of a breakthrough\n",
    "first_day = simulated_df['datetime'].min().date()\n",
    "last_day = simulated_df['datetime'].max().date()\n",
    "num_days = (last_day - first_day).days + 1\n",
    "\n",
    "breaktroughs_prbability = (simulated_df['breakthrough'] == 1).sum() / num_days\n",
    "breaktroughs_prbability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_df['breakthrough'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 5m car driving 60 will be in this zone for: \n",
    "velocity = 60 / 3.6\n",
    "print('velocity:', velocity, 'm/s')\n",
    "danger_time = 5 / velocity\n",
    "print('danger time: ', danger_time, 's')\n",
    "# with 1200 cars a day this will be that amount of seconds in danger:\n",
    "total_danger_time = 1200 * danger_time\n",
    "print('total danger time:', total_danger_time, 's')\n",
    "# precentage of cars being in danger per day:\n",
    "danger_time_proportion = total_danger_time / (24 * 60 * 60)\n",
    "print('danger time proportion: ', danger_time_proportion * 100, '%')\n",
    "\n",
    "# how likely is it that a car will be in danger and the net will break trough?\n",
    "dead_probability = breaktroughs_prbability * danger_time_proportion\n",
    "print('dead probability:', dead_probability * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(simulated_df['breakthrough'] == 1).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_combined():\n",
    "    #simulated zones\n",
    "    sim1 = simulate_zone(zone1)\n",
    "    sim1['zone'] = 1\n",
    "    sim2 = simulate_zone(zone2)\n",
    "    sim2['zone'] = 2\n",
    "    sim1.describe()\n",
    "\n",
    "    # Determine the latest end datetime of the two dataframes\n",
    "    max_datetime = min(sim1['datetime'].max(), sim2['datetime'].max())\n",
    "\n",
    "    # Set the end datetime of both dataframes to be the same\n",
    "    sim1 = sim1[sim1['datetime'] <= max_datetime]\n",
    "    sim2 = sim2[sim2['datetime'] <= max_datetime]\n",
    "\n",
    "    # Merge the two dataframes together, sort by datetime, and reset the index\n",
    "    simulated_df = pd.concat([sim1, sim2])\n",
    "    simulated_df = simulated_df.sort_values('datetime')\n",
    "    simulated_df = simulated_df.reset_index(drop=True)\n",
    "    \n",
    "    # add a column that calculates the cumulative kg already in the net.\n",
    "    # first group the data by date\n",
    "    grouped_df = simulated_df.groupby(simulated_df['datetime'].dt.date)\n",
    "\n",
    "    # then calculate the cumulative sum of 'kg' within each group \n",
    "    simulated_df['cumulative_kg'] = grouped_df['kg'].cumsum()\n",
    "    # and subtract the 'kg' valueof the new stone to get the weight in the net\n",
    "    simulated_df['cumulative_kg'] =  simulated_df['cumulative_kg'] - simulated_df['kg']\n",
    "\n",
    "    # Add a new column 'breakthrough' and set it to 1 where conditions are met\n",
    "    simulated_df['breakthrough'] = 0\n",
    "    condition1 = (simulated_df['kj'] > 1000)\n",
    "    condition2 = (simulated_df['cumulative_kg'] > 2000) & (simulated_df['kj'] > 500)\n",
    "    simulated_df.loc[condition1 | condition2, 'breakthrough'] = 1\n",
    "\n",
    "    # Calculate days passed\n",
    "    first_day = simulated_df['datetime'].min().date()\n",
    "    last_day = simulated_df['datetime'].max().date()\n",
    "    num_days = (last_day - first_day).days + 1\n",
    "    \n",
    "    breakthroughs = simulated_df['breakthrough'].sum()\n",
    "    \n",
    "    return breakthroughs, num_days, simulated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_10000_years():\n",
    "    breakthroughs = 0\n",
    "    num_days = 0\n",
    "    while num_days < 3650000:\n",
    "        breakthroughs += simulate_combined()[0]\n",
    "        num_days += simulate_combined()[1]\n",
    "    probability = breakthroughs / num_days\n",
    "    return breakthroughs, num_days, probability\n",
    "\n",
    "simulate_10000_years()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3711193/356"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1190015457/8760"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "365.25*24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steinschlag-t3PavukW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

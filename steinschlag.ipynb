{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "from scipy.stats import expon, norm, gamma, beta, lognorm\n",
    "from scipy.stats._continuous_distns import beta_gen, gamma_gen\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "## Daten einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_non_visible(s):\n",
    "    \"\"\"Strip if s is a string, otherwise return s.\"\"\"\n",
    "    if isinstance(s, str):\n",
    "        return s.strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def read_data(file):\n",
    "    \"\"\"Reads the first 4 columns from the given file and drops empty rows.\"\"\"\n",
    "    data = pd.read_csv(\n",
    "        file, delimiter=\",\", usecols=[0, 1, 2, 3], parse_dates=[[0, 1]]\n",
    "    )\n",
    "    data.columns = [\"datetime\", \"kg\", \"m/s\"]\n",
    "    data = data[data[\"datetime\"] != \"nan nan\"]\n",
    "    data = data.dropna(how=\"all\")\n",
    "    data[\"datetime\"] = pd.to_datetime(data[\"datetime\"])\n",
    "    data = data.applymap(trim_non_visible)\n",
    "    return data.sort_values(by=[\"datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1 = read_data(\"data/out_1.csv\")\n",
    "zone2 = read_data(\"data/out_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([zone1, zone2], axis=1, keys=[\"zone1\", \"zone2\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zone2.isna().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "print(zone1.isna().sum())\n",
    "\n",
    "# Check for zeros\n",
    "print(zone1.eq(0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop lines with only NA values\n",
    "zone1 = zone1.dropna(how=\"all\")\n",
    "print(zone1.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "print(zone2.isna().sum())\n",
    "\n",
    "# Check for zeros\n",
    "print(zone2.eq(0).sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Stein hat keine Masse und muss beachtet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop lines with only NA values\n",
    "zone2 = zone2.dropna(how=\"all\")\n",
    "print(zone2.isna().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stein mit 0 Masse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the value to the median an 1 in the row where 'kg' equals 0.0 to check the difference it makes\n",
    "zoneX = zone2.copy()\n",
    "zone2.loc[zone2[\"kg\"] == 0.0, \"kg\"] = zone2[\"kg\"].median()\n",
    "zoneX.loc[zoneX[\"kg\"] == 0.0, \"kg\"] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([zone2, zoneX], axis=1, keys=[\"zone2\", \"zoneX\"]).describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ob wir die 0.0 Werte durch den Median oder durch 1 ersetzen macht keinen Unterschied. Die Werte sind in beiden Fällen sehr änlich.\n",
    "wir haben uns für den Median entschieden weil wir es für wahrscheinlich halten, dass die Notiz vergessen wurde. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zone2.eq(0).sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energie und Zeitdifferenz berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_time_differences(df):\n",
    "    \"\"\"Returns the time differences between rocks in hours.\"\"\"\n",
    "    return df[\"datetime\"].diff().dt.total_seconds() / 3600\n",
    "\n",
    "\n",
    "def add_time_differences(df):\n",
    "    \"\"\"Adds the time differences to the dataframe.\"\"\"\n",
    "    df[\"timediv h\"] = _get_time_differences(df)\n",
    "\n",
    "    # replace the values in the first row of the \"timediv h\" column with the mean value\n",
    "    df.loc[0, \"timediv h\"] = df[\"timediv h\"].median()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_energy(df):\n",
    "    \"\"\"Adds the energy to the dataframe.\"\"\"\n",
    "    df[\"kj\"] = 0.5 * df[\"kg\"] * df[\"m/s\"] ** 2 / 1000\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1 = add_time_differences(add_energy(zone1))\n",
    "zone2 = add_time_differences(add_energy(zone2))\n",
    "print(zone1.head())\n",
    "print(zone2.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(zone1[\"kg\"], zone1[\"m/s\"], c=\"red\", label=\"zone1\")\n",
    "ax.scatter(zone2[\"kg\"], zone2[\"m/s\"], c=\"blue\", label=\"zone2\")\n",
    "ax.legend()\n",
    "ax.axes.set_xlabel(\"Mass [kg]\")\n",
    "ax.axes.set_ylabel(\"Velocity [m/s]\")\n",
    "n = zone1.shape[0] + zone2.shape[0]\n",
    "plt.title(f\"Mass vs Velocity in both Zones\\nNumber of records: {n}\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zonen 1 und 2 sollten nicht gemischt werden, da sie nicht teil von der gleichen Grundgesamtheit sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(\n",
    "    df: pd.DataFrame,\n",
    "    col: str,\n",
    "    colorbar=False,\n",
    "    title=None,\n",
    "    xlabel=None,\n",
    "    ylabel=None,\n",
    "):\n",
    "    \"\"\"Plots the given column of the given data frame as a scatter plot.\"\"\"\n",
    "    if title is None:\n",
    "        title = f\"{col.upper()} vs. Date\"\n",
    "    if xlabel is None:\n",
    "        xlabel = \"Date\"\n",
    "    title = title + f\"\\nnumber of records: {len(df)}\"\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "    ax = df.plot.scatter(x=\"datetime\", y=col, colorbar=colorbar)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n",
    "    ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    plt.title(\n",
    "        f\"{col} vs. date\\nnumber of records: {len(df)}\"\n",
    "        if title is None\n",
    "        else title\n",
    "    )\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "def plot_histogram(df: pd.DataFrame, col: str, zone: int, title: str = None):\n",
    "    \"\"\"Plots the given column of the given dataframe as a histogram.\"\"\"\n",
    "    if title is None:\n",
    "        title = f\"{col.upper()} for Zone {zone}\"\n",
    "    title = title + f\"\\nnumber of records: {len(df)}\"\n",
    "    df[col].hist(bins=np.sqrt(len(df[col])).astype(int) * 6)\n",
    "    plt.xlabel(col.upper())\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    zone1, \"kj\", title=\"Energy vs. Date in Zone 1\", ylabel=\"Energy [kj]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(\n",
    "    zone2, \"kj\", title=\"Energy vs. Date in Zone 2\", ylabel=\"Energy [kj]\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energie der zweiten Zone ist höher da es schnellere Steine sind. Dies ist der Fall, obwohl sie leichter sind, da die Geschwindigkeit quadratisch in die Energie eingeht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 10\n",
    "\n",
    "# Plot the histograms for 'kg'\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.hist(zone1[\"kg\"], bins=num_bins, color=\"red\", label=\"Zone 1\")\n",
    "ax2.hist(zone2[\"kg\"], bins=num_bins, color=\"blue\", label=\"Zone 2\")\n",
    "\n",
    "ax1.set_xlabel(\"Mass [kg]\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.legend()\n",
    "ax2.set_xlabel(\"Mass [kg]\")\n",
    "ax2.legend()\n",
    "plt.suptitle(\"Distribution of Mass\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the histograms for 'kj'\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.hist(zone1[\"kj\"], bins=num_bins, color=\"red\", label=\"Zone 1\")\n",
    "ax2.hist(zone2[\"kj\"], bins=num_bins, color=\"blue\", label=\"Zone 2\")\n",
    "\n",
    "ax1.set_xlabel(\"Energy [kj]\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.legend()\n",
    "ax2.set_xlabel(\"Energy [kj]\")\n",
    "ax2.legend()\n",
    "plt.suptitle(\"Distribution of Energy\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the histograms for 'm/s'\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.hist(zone1[\"m/s\"], bins=num_bins, color=\"red\", label=\"Zone 1\")\n",
    "ax2.hist(zone2[\"m/s\"], bins=num_bins, color=\"blue\", label=\"Zone 2\")\n",
    "\n",
    "ax1.set_xlabel(\"Velocity [m/s]\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.legend()\n",
    "ax2.set_xlabel(\"Velocity [m/s]\")\n",
    "ax2.legend()\n",
    "plt.suptitle(\"Distribution of Velocity\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the histograms for 'timediv h'\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.hist(zone1[\"timediv h\"], bins=num_bins, color=\"red\", label=\"Zone 1\")\n",
    "ax2.hist(zone2[\"timediv h\"], bins=num_bins, color=\"blue\", label=\"Zone 2\")\n",
    "\n",
    "ax1.set_xlabel(\"Time Difference (h)\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.legend()\n",
    "ax2.set_xlabel(\"Time Difference (h)\")\n",
    "ax2.legend()\n",
    "plt.suptitle(\"Distribution of Time Difference\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([zone1, zone2], axis=1, keys=[\"zone1\", \"zone2\"]).describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle Steine haben eine Masse und eine Geschwindigkeit. Die Maximale Energie liegt bei ~394 was noch über 100kj unter dem Grenzwert liegt, welcher bei vollem Netz definiert wurde."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse der Verteilungen der Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdf_fit(data, title, xlabel):\n",
    "    title = \"Comulative Probability of \" + title\n",
    "    distributions = [ss.norm, ss.lognorm, ss.expon, ss.gamma]\n",
    "\n",
    "    # Plot the CDF of the data and the fitted distributions\n",
    "    plt.hist(\n",
    "        data,\n",
    "        bins=len(data),\n",
    "        density=True,\n",
    "        cumulative=True,\n",
    "        alpha=0.5,\n",
    "        label=\"Data\",\n",
    "    )\n",
    "    x = np.linspace(data.min(), data.max() * 1.2, 100)\n",
    "\n",
    "    for dist in distributions:\n",
    "        params = dist.fit(data)\n",
    "        ll = -dist.logpdf(data, *params).sum().round(0)\n",
    "        plt.plot(x, dist(*params).cdf(x), label=f\"{dist.name}, score: {ll}\")\n",
    "        plt.xlabel(data.name)\n",
    "        plt.ylabel(\"Cumulative probability\")\n",
    "        plt.legend()\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_fit(zone1[\"kg\"], \"Mass of Zone 1\", \"Mass [kg]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_fit(zone2[\"kg\"], \"Mass of Zone 2\", \"Mass [kg]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_fit(zone1[\"m/s\"], \"Velocity of Zone 1\", \"Velocity [m/s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_fit(zone2[\"m/s\"], \"Velocity of Zone 2\", \"Velocity [m/s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_fit(zone1[\"timediv h\"], \"Time Difference of Zone 1\", \"Time Difference [h]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_fit(zone2[\"timediv h\"], \"Time Difference of Zone 2\", \"Time Difference [h]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to simulate a dataframe for the next number of years it will estimate the number of events it takes and generate a frame\n",
    "def simulate_zone(zone_df, num_years=200):\n",
    "    timediv_mean = zone_df[\"timediv h\"].mean()\n",
    "\n",
    "    # Calculate  the deviation\n",
    "    timediv_params = expon.fit(zone_df[\"timediv h\"])\n",
    "    kg_params = gamma.fit(zone_df[\"kg\"])\n",
    "    v_params = norm.fit(zone_df[\"m/s\"])\n",
    "    # Calculate number of observations for given number of years\n",
    "    total_hours = num_years * 365.25 * 24\n",
    "    num_observations = int(total_hours / timediv_mean)\n",
    "\n",
    "    # Set the starting datetime to January 1st, 2000, 00:00:00\n",
    "    current_datetime = datetime(2000, 1, 1, 0, 0, 0)\n",
    "\n",
    "    # Initialize the new dataframe and generate the data\n",
    "    simulated_df = pd.DataFrame(index=range(num_observations))\n",
    "    simulated_df[\"timediv h\"] = (\n",
    "        expon(*timediv_params).rvs(size=num_observations).round(0)\n",
    "    )\n",
    "    simulated_df[\"datetime\"] = (\n",
    "        simulated_df[\"timediv h\"]\n",
    "        .cumsum()\n",
    "        .apply(lambda x: current_datetime + timedelta(hours=x))\n",
    "    )\n",
    "    simulated_df[\"kg\"] = gamma(*kg_params).rvs(size=num_observations).round(0)\n",
    "    simulated_df[\"m/s\"] = norm(*v_params).rvs(size=num_observations).round(1)\n",
    "    simulated_df[\"kj\"] = (\n",
    "        0.5 * simulated_df[\"kg\"] * (simulated_df[\"m/s\"] ** 2) / 1000\n",
    "    )\n",
    "\n",
    "    return simulated_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auf diesem [Artikel der BAZ](https://www.bazonline.ch/autos-werden-immer-breiter-und-laenger-288912673833) stützen wir unsere Annahme dass Autos durchschnittlich 4.4m lang sind. Auf diesem [Artikel](https://www.sciencedirect.com/science/article/abs/pii/S0378437102014577) stützen wir die Annahme das die mittlere Reaktionszeit \\~1s ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 4.4m car driving 60 will be in this zone for:\n",
    "def calculate_danger_time(velocity=(60 / 3.6), length=4.4, reaction_time=1):\n",
    "    print(\"velocity:\", velocity, \"m/s\")\n",
    "    # danger_time_per_car is calculated by the length of the car divided by the velocity plus the reaction time\n",
    "    danger_time_per_car = length / velocity + reaction_time\n",
    "    print(\"danger time: \", danger_time_per_car, \"s\")\n",
    "    # with 1200 cars a day this will be that amount of seconds in danger:\n",
    "    total_danger_time = 1200 * danger_time_per_car\n",
    "    print(\"total danger time:\", total_danger_time, \"s\")\n",
    "    # precentage of cars being in danger per day:\n",
    "    danger_time_proportion = total_danger_time / (24 * 60 * 60)\n",
    "    print(\"danger time proportion: \", danger_time_proportion * 100, \"%\")\n",
    "    return danger_time_proportion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test der Sumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulated zones\n",
    "simulated1 = simulate_zone(zone1)\n",
    "simulated1[\"zone\"] = 1\n",
    "simulated2 = simulate_zone(zone2)\n",
    "simulated2[\"zone\"] = 2\n",
    "simulated1.describe()\n",
    "# m/s is the min 0 which is not possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([zone1, simulated1], axis=1, keys=[\"zone1\", \"simulated1\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([zone2, simulated2], axis=1, keys=[\"zone2\", \"simulated2\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables\n",
    "num_bins = 200\n",
    "cumulative = True\n",
    "\n",
    "# Plot the histograms for 'kg', 'm/s', and 'timediv h'\n",
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(\n",
    "    3, 2, figsize=(10, 10)\n",
    ")\n",
    "\n",
    "# Histogram for 'kg'\n",
    "ax1.hist(\n",
    "    zone1[\"kg\"],\n",
    "    bins=num_bins,\n",
    "    color=\"red\",\n",
    "    alpha=0.5,\n",
    "    label=\"Zone 1\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "ax1.hist(\n",
    "    simulated1[\"kg\"],\n",
    "    bins=num_bins,\n",
    "    color=\"orange\",\n",
    "    alpha=0.5,\n",
    "    label=\"Simulated 1\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "ax2.hist(\n",
    "    zone2[\"kg\"],\n",
    "    bins=num_bins,\n",
    "    color=\"blue\",\n",
    "    alpha=0.5,\n",
    "    label=\"Zone 2\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "ax2.hist(\n",
    "    simulated2[\"kg\"],\n",
    "    bins=num_bins,\n",
    "    color=\"green\",\n",
    "    alpha=0.5,\n",
    "    label=\"Simulated 2\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "\n",
    "ax1.set_xlabel(\"kg\")\n",
    "ax1.set_ylabel(\"Frequency Density\")\n",
    "ax1.legend()\n",
    "ax1.set_title(\"Simulated vs Empiric Mass - Zone 1\")\n",
    "ax2.set_xlabel(\"kg\")\n",
    "ax2.legend()\n",
    "ax2.set_title(\"Simulated vs Empiric Mass - Zone 2\")\n",
    "\n",
    "# Histogram for 'm/s'\n",
    "ax3.hist(\n",
    "    zone1[\"m/s\"],\n",
    "    bins=num_bins,\n",
    "    color=\"red\",\n",
    "    alpha=0.5,\n",
    "    label=\"Zone 1\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "ax3.hist(\n",
    "    simulated1[\"m/s\"],\n",
    "    bins=num_bins,\n",
    "    color=\"orange\",\n",
    "    alpha=0.5,\n",
    "    label=\"Simulated 1\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "ax4.hist(\n",
    "    zone2[\"m/s\"],\n",
    "    bins=num_bins,\n",
    "    color=\"blue\",\n",
    "    alpha=0.5,\n",
    "    label=\"Zone 2\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "ax4.hist(\n",
    "    simulated2[\"m/s\"],\n",
    "    bins=num_bins,\n",
    "    color=\"green\",\n",
    "    alpha=0.5,\n",
    "    label=\"Simulated 2\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "\n",
    "ax3.set_xlabel(\"m/s\")\n",
    "ax3.set_ylabel(\"Frequency Density\")\n",
    "ax3.legend()\n",
    "ax3.set_title(\"Simulated vs Empiric Velocity - Zone 1\")\n",
    "ax4.set_xlabel(\"m/s\")\n",
    "ax4.legend()\n",
    "ax4.set_title(\"Simulated vs Empiric Velocity - Zone 2\")\n",
    "\n",
    "# Histogram for 'timediv h'\n",
    "ax5.hist(\n",
    "    zone1[\"timediv h\"],\n",
    "    bins=num_bins,\n",
    "    color=\"red\",\n",
    "    alpha=0.5,\n",
    "    label=\"Zone 1\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "ax5.hist(\n",
    "    simulated1[\"timediv h\"],\n",
    "    bins=num_bins,\n",
    "    color=\"orange\",\n",
    "    alpha=0.5,\n",
    "    label=\"Simulated 1\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "ax6.hist(\n",
    "    zone2[\"timediv h\"],\n",
    "    bins=num_bins,\n",
    "    color=\"blue\",\n",
    "    alpha=0.5,\n",
    "    label=\"Zone 2\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "ax6.hist(\n",
    "    simulated2[\"timediv h\"],\n",
    "    bins=num_bins,\n",
    "    color=\"green\",\n",
    "    alpha=0.5,\n",
    "    label=\"Simulated 2\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "\n",
    "ax5.set_xlabel(\"timediv h\")\n",
    "ax5.set_ylabel(\"Frequency Density\")\n",
    "ax5.legend()\n",
    "ax5.set_title(\"Simulated vs Empiric Time Difference - Zone 1\")\n",
    "ax6.set_xlabel(\"timediv h\")\n",
    "ax6.legend()\n",
    "ax6.set_title(\"Simulated vs Empiric Time Difference - Zone 2\")\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the latest end datetime of the two dataframes\n",
    "max_datetime = min(simulated1[\"datetime\"].max(), simulated2[\"datetime\"].max())\n",
    "\n",
    "# Set the end datetime of both dataframes to be the same\n",
    "simulated1 = simulated1[simulated1[\"datetime\"] <= max_datetime]\n",
    "simulated2 = simulated2[simulated2[\"datetime\"] <= max_datetime]\n",
    "\n",
    "# Merge the two dataframes together, sort by datetime, and reset the index\n",
    "simulated_df = pd.concat([simulated1, simulated2])\n",
    "simulated_df = simulated_df.sort_values(\"datetime\")\n",
    "simulated_df = simulated_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i look at the tail to make shure the dates got calculated correctly.\n",
    "simulated_df.tail(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because the reaction time is 24h we will asume that the nets will get emptied every evening if there are stones in it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column that calculates the cumulative kg already in the net.\n",
    "\n",
    "# first group the data by date\n",
    "grouped_df = simulated_df.groupby(simulated_df[\"datetime\"].dt.date)\n",
    "\n",
    "# then calculate the cumulative sum of 'kg' within each group\n",
    "simulated_df[\"cumulative_kg\"] = grouped_df[\"kg\"].cumsum()\n",
    "# and subtract the 'kg' valueof the new stone to get the weight in the net\n",
    "simulated_df[\"cumulative_kg\"] = (\n",
    "    simulated_df[\"cumulative_kg\"] - simulated_df[\"kg\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should we disregard the rest stones of the day if the net broke trough?\n",
    "# after this the road probably gets closed\n",
    "\n",
    "# Add a new column 'breakthrough'\n",
    "simulated_df[\"breakthrough\"] = 0\n",
    "\n",
    "# Set breakthrough to 1 where conditions are met\n",
    "condition1 = simulated_df[\"kj\"] > 1000\n",
    "condition2 = (simulated_df[\"cumulative_kg\"] > 2000) & (\n",
    "    simulated_df[\"kj\"] > 500\n",
    ")\n",
    "simulated_df.loc[condition1 | condition2, \"breakthrough\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i look at the tail to make sure the cumulative_kg and breakthrough got calculated correctly.\n",
    "\n",
    "simulated_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability of a breakthrough\n",
    "first_day = simulated_df[\"datetime\"].min().date()\n",
    "last_day = simulated_df[\"datetime\"].max().date()\n",
    "num_days = (last_day - first_day).days + 1\n",
    "\n",
    "breaktroughs_prbability = (simulated_df[\"breakthrough\"] == 1).sum() / num_days\n",
    "breaktroughs_prbability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_df[\"breakthrough\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "danger_time_proportion = calculate_danger_time()\n",
    "\n",
    "# how likely is it that a car will be in danger and the net will break trough?\n",
    "dead_probability = breaktroughs_prbability * danger_time_proportion\n",
    "print(\"dead probability:\", dead_probability * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(simulated_df[\"breakthrough\"] == 1).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Durchführung der Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_combined():\n",
    "    # simulated zones\n",
    "    sim1 = simulate_zone(zone1)\n",
    "    sim1[\"zone\"] = 1\n",
    "    sim2 = simulate_zone(zone2)\n",
    "    sim2[\"zone\"] = 2\n",
    "    sim1.describe()\n",
    "\n",
    "    # Determine the latest end datetime of the two dataframes\n",
    "    max_datetime = min(sim1[\"datetime\"].max(), sim2[\"datetime\"].max())\n",
    "\n",
    "    # Set the end datetime of both dataframes to be the same\n",
    "    sim1 = sim1[sim1[\"datetime\"] <= max_datetime]\n",
    "    sim2 = sim2[sim2[\"datetime\"] <= max_datetime]\n",
    "\n",
    "    # Merge the two dataframes together, sort by datetime, and reset the index\n",
    "    simulated_df = pd.concat([sim1, sim2])\n",
    "    simulated_df = simulated_df.sort_values(\"datetime\")\n",
    "    simulated_df = simulated_df.reset_index(drop=True)\n",
    "\n",
    "    # add a column that calculates the cumulative kg already in the net.\n",
    "    # first group the data by date\n",
    "    grouped_df = simulated_df.groupby(simulated_df[\"datetime\"].dt.date)\n",
    "\n",
    "    # then calculate the cumulative sum of 'kg' within each group\n",
    "    simulated_df[\"cumulative_kg\"] = grouped_df[\"kg\"].cumsum()\n",
    "    # and subtract the 'kg' valueof the new stone to get the weight in the net\n",
    "    simulated_df[\"cumulative_kg\"] = (\n",
    "        simulated_df[\"cumulative_kg\"] - simulated_df[\"kg\"]\n",
    "    )\n",
    "\n",
    "    # Add a new column 'breakthrough' and set it to 1 where conditions are met\n",
    "    simulated_df[\"breakthrough\"] = 0\n",
    "    condition1 = simulated_df[\"kj\"] > 1000\n",
    "    condition2 = (simulated_df[\"cumulative_kg\"] > 2000) & (\n",
    "        simulated_df[\"kj\"] > 500\n",
    "    )\n",
    "    simulated_df.loc[condition1 | condition2, \"breakthrough\"] = 1\n",
    "\n",
    "    # Calculate days passed\n",
    "    first_day = simulated_df[\"datetime\"].min().date()\n",
    "    last_day = simulated_df[\"datetime\"].max().date()\n",
    "    num_days = (last_day - first_day).days + 1\n",
    "\n",
    "    breakthroughs = simulated_df[\"breakthrough\"].sum()\n",
    "\n",
    "    return breakthroughs, num_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_years(years=10000):\n",
    "    breakthroughs = 0\n",
    "    num_days = 0\n",
    "    while num_days < (years * 365):\n",
    "        a, b = simulate_combined()\n",
    "        breakthroughs += a\n",
    "        num_days += b\n",
    "    probability = breakthroughs / num_days\n",
    "    return probability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welche Zeitdauer sollte simuliert werden?\n",
    "\n",
    "In diesem Abschnitt wird untersucht ob die Simulation konvergiert. Da dieser Teil sehr lange dauert, wird er nur einmal ausgeführt und das Resultat wird gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation_until_convergence(\n",
    "    years=100_000, threshold=0.000001, max_years=1_000_000, cnt_max=5\n",
    "):\n",
    "    prev_prob, cnt = 100, 0\n",
    "    while years < max_years:\n",
    "        curr_prob = simulate_years(years)\n",
    "        cnt = cnt + 1 if abs(curr_prob - prev_prob) < threshold else 0\n",
    "        print(f\"After {years} years the car hit probability is {curr_prob}\")\n",
    "        if cnt >= cnt_max:\n",
    "            print(\n",
    "                f\"Converged after {years} years with a car hit probability of {curr_prob}\"\n",
    "            )\n",
    "            return curr_prob\n",
    "\n",
    "        prev_prob = curr_prob\n",
    "        years += 100_000\n",
    "    print(\n",
    "        f\"stopped after {years} (max: {max_years}) without convergence. Final probability: {curr_prob}\"\n",
    "    )\n",
    "    return curr_prob\n",
    "\n",
    "\n",
    "# run_simulation_until_convergence()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First version:\n",
    "```\n",
    "After 1000 years the car hit probability is 3.561263317070231e-05\n",
    "After 2000 years the car hit probability is 5.364693083413492e-05\n",
    "After 4000 years the car hit probability is 3.405758089002938e-05\n",
    "After 8000 years the car hit probability is 4.217577456981547e-05\n",
    "After 16000 years the car hit probability is 4.3377030608425545e-05\n",
    "After 32000 years the car hit probability is 4.1614637710042e-05\n",
    "After 64000 years the car hit probability is 4.30908377539241e-05\n",
    "After 128000 years the car hit probability is 4.318400823489107e-05\n",
    "After 256000 years the car hit probability is 4.175662569138142e-05\n",
    "Converged after 256000 years with a car hit probability of 4.175662569138142e-05\n",
    "```\n",
    "\n",
    "second version:\n",
    "```\n",
    "After 100000 years the car hit probability is 4.370946770867339e-05\n",
    "After 200000 years the car hit probability is 4.0979629962153794e-05\n",
    "After 300000 years the car hit probability is 4.304146962984774e-05\n",
    "After 400000 years the car hit probability is 4.2160043179880796e-05\n",
    "After 500000 years the car hit probability is 4.128412826292135e-05\n",
    "After 600000 years the car hit probability is 4.216004440371104e-05\n",
    "After 700000 years the car hit probability is 4.218535823026752e-05\n",
    "After 800000 years the car hit probability is 4.122875384867198e-05\n",
    "Converged after 800000 years with a car hit probability of 4.122875384867198e-05\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Simulation\n",
    "\n",
    "[link](https://www.eea.europa.eu/data-and-maps/figures/term29-occupancy-rates-in-passenger-transport-1) zeigt dass in der Schweiz im Jahr 2008 durchschnittlich ~1.5 Passiere pro Auto sitzen. Wir nehmen an dass dies auch heute noch so ist. Die Annahme dass man einen 50% Wahrscheinlichkeit hat zu überleben bei einer Kollision mit einer Wand (Felsen von über 2t vergleichen wir mit einer Wand) entnehmen wir diesem [Artikel](https://www.sciencedirect.com/science/article/abs/pii/S0001457519301058). Den Bremsweg haben wir vernachlässigt, weil wir den Grenzwert von 0.0001 schon überschritten haben. Würde man den Bremsweg berücksichtigen würde, würde die Wahrscheinlichkeit, dass jemand stirbt, weiter steigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how likely is it that a car will be in danger and the net will break trough?\n",
    "print(\"danger time per day:\", danger_time_proportion * 100, \"%\")\n",
    "p_rock_breaks_net_per_day = simulate_years(10_000)\n",
    "print(\"rock breaks net per day:\", p_rock_breaks_net_per_day * 100, \"%\")\n",
    "p_car_hit_per_day = p_rock_breaks_net_per_day * danger_time_proportion\n",
    "print(\"car hit per day:\", p_car_hit_per_day * 100, \"%\")\n",
    "p_car_hit_next_year = p_car_hit_per_day * 365\n",
    "print(\"car hit next year:\", p_car_hit_next_year * 100, \"%\")\n",
    "mean_number_of_passengers, p_dying_during_crash = 1.5, 0.5\n",
    "print(\"mean number of passengers:\", mean_number_of_passengers)\n",
    "print(\n",
    "    \"probability of dying during crash at 60 km/h:\",\n",
    "    p_dying_during_crash * 100,\n",
    "    \"%\",\n",
    ")\n",
    "p_dead_person_next_year = (\n",
    "    p_car_hit_next_year * mean_number_of_passengers * p_dying_during_crash\n",
    ")\n",
    "print(\n",
    "    \"Probability that one person dies next year:\",\n",
    "    p_dead_person_next_year * 100,\n",
    "    \"%\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steinschlag-t3PavukW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

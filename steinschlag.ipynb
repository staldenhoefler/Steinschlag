{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "from scipy.stats import expon, norm, gamma, beta, lognorm\n",
    "from scipy.stats._continuous_distns import beta_gen, gamma_gen\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1 = pd.read_csv(\n",
    "    \"data/out_1.csv\",\n",
    "    usecols=[\"Datum\", \"Uhrzeit\", \"Masse [kg]\", \"Geschwindigkeit [m/s]\"],\n",
    ")\n",
    "zone2 = pd.read_csv(\n",
    "    \"data/out_2.csv\", usecols=[\"Date\", \"Uhrzeit\", \"m [kg]\", \"v [m/s]\"]\n",
    ")\n",
    "zone1.columns = [\"date\", \"time\", \"kg\", \"m/s\"]\n",
    "zone2.columns = [\"date\", \"time\", \"kg\", \"m/s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([zone1, zone2], axis=1, keys=[\"zone1\", \"zone2\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "print(zone1.isna().sum())\n",
    "\n",
    "# Check for zeros\n",
    "print(zone1.eq(0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop lines with only NA values\n",
    "zone1 = zone1.dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "print(zone2.isna().sum())\n",
    "\n",
    "# Check for zeros\n",
    "print(zone2.eq(0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop lines with only NA values\n",
    "zone2 = zone2.dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the value to the median in the row where 'kg' equals 0.0\n",
    "# This rock is not removed from the data, because we do not have a lot of data and this might just have been an error in the measurement\n",
    "zone2.loc[zone2[\"kg\"] == 0.0, \"kg\"] = zone2[\"kg\"].median()\n",
    "# TODO: try to drop the row and check if there is a difference? or go up to 1? -> how do the parameters change (don't run simulation)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([zone1, zone2], axis=1, keys=[\"zone1\", \"zone2\"]).describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1.plot(\n",
    "    x=\"kg\",\n",
    "    y=\"m/s\",\n",
    "    kind=\"scatter\",\n",
    "    c=\"red\",\n",
    "    label=\"zone1\",\n",
    "    title=\"Mass vs Velocity Zone 1\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone2.plot(\n",
    "    x=\"kg\",\n",
    "    y=\"m/s\",\n",
    "    kind=\"scatter\",\n",
    "    c=\"blue\",\n",
    "    label=\"zone2\",\n",
    "    title=\"Mass vs Velocity Zone 2\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(zone1[\"kg\"], zone1[\"m/s\"], c=\"red\", label=\"zone1\")\n",
    "ax.scatter(zone2[\"kg\"], zone2[\"m/s\"], c=\"blue\", label=\"zone2\")\n",
    "ax.legend()\n",
    "ax.axes.set_xlabel(\"m [kg]\")\n",
    "ax.axes.set_ylabel(\"v [m/s]\")\n",
    "plt.title(\"Mass vs Velocity in both Zones\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zonen 1 und 2 sollten nicht gemischt werden, weil sonst sehr schwere steine generiert werden könnten die eine hohe Geschwindigkeit haben, was in den Aufzeichnungen aber nicht vorkomt (nicht gleiche Grundgesammtheit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1[\"kj\"] = 0.5 * zone1[\"kg\"] * (zone1[\"m/s\"] ** 2) / 1000\n",
    "zone2[\"kj\"] = 0.5 * zone2[\"kg\"] * (zone2[\"m/s\"] ** 2) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' and 'time' columns to datetime for zone1\n",
    "zone1[\"datetime\"] = pd.to_datetime(zone1[\"date\"] + \" \" + zone1[\"time\"])\n",
    "\n",
    "# Calculate the time difference between each row and the row above, in hours then set the first row to the median of all rows\n",
    "zone1[\"timediv h\"] = (zone1[\"datetime\"] - zone1[\"datetime\"].shift()).fillna(\n",
    "    pd.Timedelta(seconds=0)\n",
    ")\n",
    "zone1[\"timediv h\"] = zone1[\"timediv h\"].apply(\n",
    "    lambda x: int(round(x.total_seconds() / 3600))\n",
    ")\n",
    "\n",
    "# Set the first row to the median of all rows so we don't loose a value\n",
    "zone1.loc[0, \"timediv h\"] = zone1[\"timediv h\"].median()\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(zone1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' and 'time' columns to datetime for zone2\n",
    "zone2[\"datetime\"] = pd.to_datetime(zone2[\"date\"] + \" \" + zone2[\"time\"])\n",
    "\n",
    "# Calculate the time difference between each row and the row above, in hours\n",
    "zone2[\"timediv h\"] = (zone2[\"datetime\"] - zone2[\"datetime\"].shift()).fillna(\n",
    "    pd.Timedelta(seconds=0)\n",
    ")\n",
    "zone2[\"timediv h\"] = zone2[\"timediv h\"].apply(\n",
    "    lambda x: int(round(x.total_seconds() / 3600))\n",
    ")\n",
    "\n",
    "# Set the first row to the median of all rows so we don't loose a value\n",
    "zone2.loc[0, \"timediv h\"] = zone2[\"timediv h\"].median()\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(zone2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 10  # Change this value to adjust the number of bins\n",
    "\n",
    "# Plot the histograms for 'kg'\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.hist(zone1[\"kg\"], bins=num_bins, color=\"red\", label=\"Zone 1\")\n",
    "ax2.hist(zone2[\"kg\"], bins=num_bins, color=\"blue\", label=\"Zone 2\")\n",
    "\n",
    "ax1.set_xlabel(\"kg\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.legend()\n",
    "ax2.set_xlabel(\"kg\")\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the histograms for 'kj'\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.hist(zone1[\"kj\"], bins=num_bins, color=\"red\", label=\"Zone 1\")\n",
    "ax2.hist(zone2[\"kj\"], bins=num_bins, color=\"blue\", label=\"Zone 2\")\n",
    "\n",
    "ax1.set_xlabel(\"kj\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.legend()\n",
    "ax2.set_xlabel(\"kj\")\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the histograms for 'm/s'\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.hist(zone1[\"m/s\"], bins=num_bins, color=\"red\", label=\"Zone 1\")\n",
    "ax2.hist(zone2[\"m/s\"], bins=num_bins, color=\"blue\", label=\"Zone 2\")\n",
    "\n",
    "ax1.set_xlabel(\"m/s\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.legend()\n",
    "ax2.set_xlabel(\"m/s\")\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the histograms for 'timediv h'\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.hist(zone1[\"timediv h\"], bins=num_bins, color=\"red\", label=\"Zone 1\")\n",
    "ax2.hist(zone2[\"timediv h\"], bins=num_bins, color=\"blue\", label=\"Zone 2\")\n",
    "\n",
    "ax1.set_xlabel(\"timediv h\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.legend()\n",
    "ax2.set_xlabel(\"timediv h\")\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "\n",
    "# TODO: property of data, not say it's distributed that way\n",
    "# TODO: overfitting distributions to data, but we need to predict the future\n",
    "# TODO: choose distributions logically, not only calculating the best fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([zone1, zone2], axis=1, keys=[\"zone1\", \"zone2\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: understan log-likelihood/MLE\n",
    "# TODO: what other strategies are there to evaluate how good distributions fit the data?\n",
    "# TODO: compare distributions VS fitting parameters of one distribution to the data: two different problems/solutions\n",
    "def cdf_fit(data):\n",
    "    distributions = [ss.norm, ss.lognorm, ss.expon, ss.gamma]\n",
    "\n",
    "    # Plot the CDF of the data and the fitted distributions\n",
    "    plt.hist(\n",
    "        data,\n",
    "        bins=len(data),\n",
    "        density=True,\n",
    "        cumulative=True,\n",
    "        alpha=0.5,\n",
    "        label=\"Data\",\n",
    "    )\n",
    "    x = np.linspace(data.min(), data.max() * 1.2, 100)\n",
    "\n",
    "    for dist in distributions:\n",
    "        params = dist.fit(data)\n",
    "        ll = -dist.logpdf(data, *params).sum().round(0)\n",
    "        plt.plot(x, dist(*params).cdf(x), label=f\"{dist.name}, score: {ll}\")\n",
    "        plt.xlabel(data.name)\n",
    "        plt.ylabel(\"Cumulative probability\")\n",
    "        plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_fit(zone1[\"kg\"])\n",
    "\n",
    "# TODO: Plot the distribution of the best fit with the distribution of the data or qqplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_fit(zone2[\"kg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_fit(zone1[\"m/s\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_fit(zone2[\"m/s\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_fit(zone1[\"timediv h\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_fit(zone1[\"timediv h\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to simulate a dataframe for the next number of years it will estimate the number of events it takes and generate a frame\n",
    "def generate_simulated_data(zone_df, num_years=200):\n",
    "    timediv_mean = zone_df[\"timediv h\"].mean()\n",
    "\n",
    "    # Calculate  the deviation\n",
    "    timediv_params = expon.fit(zone_df[\"timediv h\"])\n",
    "    kg_params = gamma.fit(zone_df[\"kg\"])\n",
    "    v_params = norm.fit(zone_df[\"m/s\"])\n",
    "    # Calculate number of observations for given number of years\n",
    "    total_hours = num_years * 365.25 * 24\n",
    "    num_observations = int(total_hours / timediv_mean)\n",
    "    # TODO: how many will be generate? print?\n",
    "    # TODO: think about generate a number of stones?\n",
    "\n",
    "    # Set the starting datetime to January 1st, 2000, 00:00:00\n",
    "    current_datetime = datetime(2000, 1, 1, 0, 0, 0)\n",
    "\n",
    "    # Initialize the new dataframe and generate the data\n",
    "    simulated_df = pd.DataFrame(index=range(num_observations))\n",
    "    simulated_df[\"timediv h\"] = (\n",
    "        expon(*timediv_params).rvs(size=num_observations).round(0)\n",
    "    )\n",
    "    simulated_df[\"datetime\"] = (\n",
    "        simulated_df[\"timediv h\"]\n",
    "        .cumsum()\n",
    "        .apply(lambda x: current_datetime + timedelta(hours=x))\n",
    "    )\n",
    "    simulated_df[\"kg\"] = gamma(*kg_params).rvs(size=num_observations).round(0)\n",
    "    simulated_df[\"m/s\"] = norm(*v_params).rvs(size=num_observations).round(1)\n",
    "    simulated_df[\"kj\"] = (\n",
    "        0.5 * simulated_df[\"kg\"] * (simulated_df[\"m/s\"] ** 2) / 1000\n",
    "    )\n",
    "\n",
    "    return simulated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulated dataframes\n",
    "simulated1 = generate_simulated_data(zone1)\n",
    "simulated1[\"zone\"] = 1\n",
    "simulated2 = generate_simulated_data(zone2)\n",
    "simulated2[\"zone\"] = 2\n",
    "simulated1.describe()\n",
    "# m/s is the min 0 which is not possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([zone1, simulated1], axis=1, keys=[\"zone1\", \"simulated1\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set variables\n",
    "num_bins = 200\n",
    "cumulative = True\n",
    "\n",
    "# Plot the histograms for 'kg', 'm/s', and 'timediv h'\n",
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(\n",
    "    3, 2, figsize=(10, 10)\n",
    ")\n",
    "\n",
    "# Histogram for 'kg'\n",
    "ax1.hist(\n",
    "    zone1[\"kg\"],\n",
    "    bins=num_bins,\n",
    "    color=\"red\",\n",
    "    alpha=0.5,\n",
    "    label=\"Zone 1\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "ax1.hist(\n",
    "    simulated1[\"kg\"],\n",
    "    bins=num_bins,\n",
    "    color=\"orange\",\n",
    "    alpha=0.5,\n",
    "    label=\"Simulated 1\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "ax2.hist(\n",
    "    zone2[\"kg\"],\n",
    "    bins=num_bins,\n",
    "    color=\"blue\",\n",
    "    alpha=0.5,\n",
    "    label=\"Zone 2\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "ax2.hist(\n",
    "    simulated2[\"kg\"],\n",
    "    bins=num_bins,\n",
    "    color=\"green\",\n",
    "    alpha=0.5,\n",
    "    label=\"Simulated 2\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "\n",
    "ax1.set_xlabel(\"kg\")\n",
    "ax1.set_ylabel(\"Frequency Density\")\n",
    "ax1.legend()\n",
    "ax2.set_xlabel(\"kg\")\n",
    "ax2.legend()\n",
    "\n",
    "# Histogram for 'm/s'\n",
    "ax3.hist(\n",
    "    zone1[\"m/s\"],\n",
    "    bins=num_bins,\n",
    "    color=\"red\",\n",
    "    alpha=0.5,\n",
    "    label=\"Zone 1\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "ax3.hist(\n",
    "    simulated1[\"m/s\"],\n",
    "    bins=num_bins,\n",
    "    color=\"orange\",\n",
    "    alpha=0.5,\n",
    "    label=\"Simulated 1\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "ax4.hist(\n",
    "    zone2[\"m/s\"],\n",
    "    bins=num_bins,\n",
    "    color=\"blue\",\n",
    "    alpha=0.5,\n",
    "    label=\"Zone 2\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "ax4.hist(\n",
    "    simulated2[\"m/s\"],\n",
    "    bins=num_bins,\n",
    "    color=\"green\",\n",
    "    alpha=0.5,\n",
    "    label=\"Simulated 2\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "\n",
    "ax3.set_xlabel(\"m/s\")\n",
    "ax3.set_ylabel(\"Frequency Density\")\n",
    "ax3.legend()\n",
    "ax4.set_xlabel(\"m/s\")\n",
    "ax4.legend()\n",
    "\n",
    "# Histogram for 'timediv h'\n",
    "ax5.hist(\n",
    "    zone1[\"timediv h\"],\n",
    "    bins=num_bins,\n",
    "    color=\"red\",\n",
    "    alpha=0.5,\n",
    "    label=\"Zone 1\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "ax5.hist(\n",
    "    simulated1[\"timediv h\"],\n",
    "    bins=num_bins,\n",
    "    color=\"orange\",\n",
    "    alpha=0.5,\n",
    "    label=\"Simulated 1\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "ax6.hist(\n",
    "    zone2[\"timediv h\"],\n",
    "    bins=num_bins,\n",
    "    color=\"blue\",\n",
    "    alpha=0.5,\n",
    "    label=\"Zone 2\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "ax6.hist(\n",
    "    simulated2[\"timediv h\"],\n",
    "    bins=num_bins,\n",
    "    color=\"green\",\n",
    "    alpha=0.5,\n",
    "    label=\"Simulated 2\",\n",
    "    density=True,\n",
    "    cumulative=cumulative,\n",
    ")\n",
    "\n",
    "ax5.set_xlabel(\"timediv h\")\n",
    "ax5.set_ylabel(\"Frequency Density\")\n",
    "ax5.legend()\n",
    "ax6.set_xlabel(\"timediv h\")\n",
    "ax6.legend()\n",
    "\n",
    "plt.show()\n",
    "# TODO: plot cumulative distribution to compare data and generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_simulated_to_original(\n",
    "    original_column, simulated_column, num_bins=200, cumulative=True\n",
    "):\n",
    "    plt.hist(\n",
    "        original_column,\n",
    "        bins=num_bins,\n",
    "        color=\"red\",\n",
    "        alpha=0.5,\n",
    "        label=\"Zone 1\",\n",
    "        density=True,\n",
    "        cumulative=cumulative,\n",
    "    )\n",
    "    plt.hist(\n",
    "        simulated_column,\n",
    "        bins=num_bins,\n",
    "        color=\"orange\",\n",
    "        alpha=0.5,\n",
    "        label=\"Simulated 1\",\n",
    "        density=True,\n",
    "        cumulative=cumulative,\n",
    "    )\n",
    "\n",
    "    ax1.set_xlabel(original_column.name)\n",
    "    ax1.set_ylabel(\"Frequency Density\")\n",
    "    ax1.legend()\n",
    "\n",
    "\n",
    "compare_simulated_to_original(zone1[\"kg\"], simulated1[\"kg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the latest end datetime of the two dataframes\n",
    "max_datetime = min(simulated1[\"datetime\"].max(), simulated2[\"datetime\"].max())\n",
    "\n",
    "# Set the end datetime of both dataframes to be the same\n",
    "simulated1 = simulated1[simulated1[\"datetime\"] <= max_datetime]\n",
    "simulated2 = simulated2[simulated2[\"datetime\"] <= max_datetime]\n",
    "\n",
    "# Merge the two dataframes together, sort by datetime, and reset the index\n",
    "simulated_df = pd.concat([simulated1, simulated2])\n",
    "simulated_df = simulated_df.sort_values(\"datetime\")\n",
    "simulated_df = simulated_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_df.tail(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because the reaction time is 24h we will asume that the nets will get emptied every evening if there are stones in it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column that calculates the cumulative kg already in the net.\n",
    "\n",
    "# first group the data by date\n",
    "grouped_df = simulated_df.groupby(simulated_df[\"datetime\"].dt.date)\n",
    "\n",
    "# then calculate the cumulative sum of 'kg' within each group\n",
    "simulated_df[\"cumulative_kg\"] = grouped_df[\"kg\"].cumsum()\n",
    "# and subtract the 'kg' valueof the new stone to get the weight in the net\n",
    "simulated_df[\"cumulative_kg\"] = (\n",
    "    simulated_df[\"cumulative_kg\"] - simulated_df[\"kg\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should we disregard the rest stones of the day if the net broke trough?\n",
    "# after this the road probably gets closed\n",
    "\n",
    "# Add a new column 'breakthrough'\n",
    "simulated_df[\"breakthrough\"] = 0\n",
    "\n",
    "# Set breakthrough to 1 where conditions are met\n",
    "# TODO: function which defines the strength of the net?\n",
    "condition1 = simulated_df[\"kj\"] > 1000\n",
    "condition2 = (simulated_df[\"cumulative_kg\"] > 2000) & (\n",
    "    simulated_df[\"kj\"] > 500\n",
    ")\n",
    "simulated_df.loc[condition1 | condition2, \"breakthrough\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i look at the tail to make shure the cumulative_kg got calculated correctly.\n",
    "\n",
    "simulated_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability of a breakthrough\n",
    "first_day = simulated_df[\"datetime\"].min().date()\n",
    "last_day = simulated_df[\"datetime\"].max().date()\n",
    "num_days = (last_day - first_day).days + 1\n",
    "\n",
    "breaktroughs_prbability = (simulated_df[\"breakthrough\"] == 1).sum() / num_days\n",
    "breaktroughs_prbability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_df[\"breakthrough\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: geschwindigkeit als zufallsvariable?\n",
    "# TODO: auto fährt in stein?\n",
    "\n",
    "# a 5m car driving 60 will be in this zone for:\n",
    "velocity = 60 / 3.6\n",
    "print(\"velocity:\", velocity, \"m/s\")\n",
    "danger_time = 5 / velocity\n",
    "print(\"danger time: \", danger_time, \"s\")\n",
    "# with 1200 cars a day this will be that amount of seconds in danger:\n",
    "total_danger_time = 1200 * danger_time\n",
    "print(\"total danger time:\", total_danger_time, \"s\")\n",
    "# precentage of cars being in danger per day:\n",
    "danger_time_proportion = total_danger_time / (24 * 60 * 60)\n",
    "print(\"danger time proportion: \", danger_time_proportion * 100, \"%\")\n",
    "\n",
    "# how likely is it that a car will be in danger and the net will break trough?\n",
    "dead_probability = breaktroughs_prbability * danger_time_proportion\n",
    "print(\"dead probability:\", dead_probability * 100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steinschlag-t3PavukW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

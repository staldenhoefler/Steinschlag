{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import  expon, norm, gamma, beta, lognorm\n",
    "from scipy.stats._continuous_distns import beta_gen, gamma_gen\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1 = pd.read_csv('data/out_1.csv', usecols=[\"Datum\", \"Uhrzeit\", \"Masse [kg]\", \"Geschwindigkeit [m/s]\"])\n",
    "zone2 = pd.read_csv('data/out_2.csv', usecols=[\"Date\", \"Uhrzeit\", \"m [kg]\", \"v [m/s]\"])\n",
    "zone1.columns = [\"date\", \"time\", \"kg\", \"m/s\"]\n",
    "zone2.columns = [\"date\", \"time\", \"kg\", \"m/s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([zone1, zone2], axis=1, keys=['zone1', 'zone2']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "print(zone1.isna().sum())\n",
    "\n",
    "# Check for zeros\n",
    "print(zone1.eq(0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop lines with only NA values\n",
    "zone1 = zone1.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "print(zone2.isna().sum())\n",
    "\n",
    "# Check for zeros\n",
    "print(zone2.eq(0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop lines with only NA values\n",
    "zone2 = zone2.dropna(how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the value to the median in the row where 'kg' equals 0.0\n",
    "# This rock is not removed from the data, because we do not have a lot of data and this might just have been an error in the measurement\n",
    "zone2.loc[zone2['kg'] == 0.0, 'kg'] = zone2['kg'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([zone1, zone2], axis=1, keys=['zone1', 'zone2']).describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1.plot(x='kg', y='m/s', kind='scatter', c='red', label='zone1', title='Mass vs Velocity Zone 1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone2.plot(x='kg', y='m/s', kind='scatter', c='blue', label='zone2', title='Mass vs Velocity Zone 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(zone1['kg'], zone1['m/s'], c='red', label='zone1')\n",
    "ax.scatter(zone2['kg'], zone2['m/s'], c='blue', label='zone2')\n",
    "ax.legend()\n",
    "ax.axes.set_xlabel('m [kg]')\n",
    "ax.axes.set_ylabel('v [m/s]')\n",
    "plt.title('Mass vs Velocity in both Zones')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zonen 1 und 2 sollten nicht gemischt werden, weil sonst sehr schwere steine generiert werden k√∂nnten die eine hohe Geschwindigkeit haben, was in den Aufzeichnungen aber nicht vorkomt (nicht gleiche Grundgesammtheit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone1['kj'] = 0.5 * zone1['kg'] * (zone1['m/s']**2) /1000\n",
    "zone2['kj'] = 0.5 * zone2['kg'] * (zone2['m/s']**2) /1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' and 'time' columns to datetime for zone1\n",
    "zone1['datetime'] = pd.to_datetime(zone1['date'] + ' ' + zone1['time'])\n",
    "\n",
    "# Calculate the time difference between each row and the row above, in hours then set the first row to the median of all rows\n",
    "zone1['timediv h'] = (zone1['datetime'] - zone1['datetime'].shift()).fillna(pd.Timedelta(seconds=0))\n",
    "zone1['timediv h'] = zone1['timediv h'].apply(lambda x: int(round(x.total_seconds() / 3600)))\n",
    "\n",
    "# Set the first row to the median of all rows so we don't loose a value\n",
    "zone1.loc[0, 'timediv h'] = zone1['timediv h'].median()\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(zone1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' and 'time' columns to datetime for zone2\n",
    "zone2['datetime'] = pd.to_datetime(zone2['date'] + ' ' + zone2['time'])\n",
    "\n",
    "# Calculate the time difference between each row and the row above, in hours\n",
    "zone2['timediv h'] = (zone2['datetime'] - zone2['datetime'].shift()).fillna(pd.Timedelta(seconds=0))\n",
    "zone2['timediv h'] = zone2['timediv h'].apply(lambda x: int(round(x.total_seconds() / 3600)))\n",
    "\n",
    "# Set the first row to the median of all rows so we don't loose a value\n",
    "zone2.loc[0, 'timediv h'] = zone2['timediv h'].median()\n",
    "\n",
    "# Print the updated dataframe\n",
    "print(zone2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 10  # Change this value to adjust the number of bins\n",
    "\n",
    "# Plot the histograms for 'kg'\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.hist(zone1['kg'], bins=num_bins, color='red', label='Zone 1')\n",
    "ax2.hist(zone2['kg'], bins=num_bins, color='blue', label='Zone 2')\n",
    "\n",
    "ax1.set_xlabel('kg')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.legend()\n",
    "ax2.set_xlabel('kg')\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the histograms for 'kj'\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.hist(zone1['kj'], bins=num_bins, color='red', label='Zone 1')\n",
    "ax2.hist(zone2['kj'], bins=num_bins, color='blue', label='Zone 2')\n",
    "\n",
    "ax1.set_xlabel('kj')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.legend()\n",
    "ax2.set_xlabel('kj')\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the histograms for 'm/s'\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.hist(zone1['m/s'], bins=num_bins, color='red', label='Zone 1')\n",
    "ax2.hist(zone2['m/s'], bins=num_bins, color='blue', label='Zone 2')\n",
    "\n",
    "ax1.set_xlabel('m/s')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.legend()\n",
    "ax2.set_xlabel('m/s')\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the histograms for 'timediv h'\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.hist(zone1['timediv h'], bins=num_bins, color='red', label='Zone 1')\n",
    "ax2.hist(zone2['timediv h'], bins=num_bins, color='blue', label='Zone 2')\n",
    "\n",
    "ax1.set_xlabel('timediv h')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.legend()\n",
    "ax2.set_xlabel('timediv h')\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([zone1, zone2], axis=1, keys=['zone1', 'zone2']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_best_distribution(data):\n",
    "    # Define candidate distributions\n",
    "    dist_names = [norm, lognorm, expon, gamma, beta]\n",
    "\n",
    "    # Set up initial best parameters and likelihoods\n",
    "    best_dist = None\n",
    "    best_params = {}\n",
    "    best_ll = 1_000_000\n",
    "\n",
    "    # Set up a dictionary to store the log-likelihoods of each distribution\n",
    "    ll_dict = {}\n",
    "\n",
    "    # Iterate through candidate distributions and find the best fit\n",
    "    for dist_name in dist_names:\n",
    "        # Fit the distribution to the data using MLE\n",
    "        params = dist_name.fit(data)\n",
    "\n",
    "        # Get the negative log-likelihood of the data under the distribution\n",
    "        ll = -dist_name.logpdf(data, *params).sum()\n",
    "\n",
    "        # If the fit is better than the current best, update the best fit\n",
    "        if ll < best_ll:\n",
    "            best_dist = dist_name\n",
    "            best_params = params\n",
    "            best_ll = ll\n",
    "\n",
    "        # Store the log-likelihood of the fit for this distribution\n",
    "        ll_dict[dist_name] = ll\n",
    "\n",
    "    # Sort the distributions by the log-likelihood of their fit\n",
    "    sorted_dists = sorted(ll_dict, key=ll_dict.get)\n",
    "\n",
    "    # Generate a list of strings describing the fit of each distribution\n",
    "    dist_strings = []\n",
    "    for dist_name in sorted_dists:\n",
    "        if dist_name == best_dist:\n",
    "            dist_strings.append(f'{dist_name} (best fit): {ll_dict[dist_name]}')\n",
    "        else:\n",
    "            dist_strings.append(f'{dist_name}: {ll_dict[dist_name]}')\n",
    "\n",
    "    # Return the best distribution, its parameters, and the list of distribution fit strings\n",
    "    return best_dist, best_params, dist_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1_kg_dist, z1_kg_param, z1_kg_results = fit_best_distribution(zone1['kg'])\n",
    "z1_kg_results\n",
    "\n",
    "# TODO: Plot the distribution of the best fit with the distribution of the data or qqplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_best_distribution(zone2['kg']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_best_distribution(zone1['m/s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_best_distribution(zone2['m/s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_best_distribution(zone1['timediv h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_best_distribution(zone1['timediv h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to simulate a dataframe for the next number of years it will estimate the number of events it takes and generate a frame \n",
    "def generate_simulated_data(zone_df, num_years=200):\n",
    "    \n",
    "    timediv_mean = zone_df['timediv h'].mean()\n",
    "    \n",
    "    # Calculate  the deviation\n",
    "    timediv_params = gamma.fit(zone_df['timediv h'])\n",
    "    kg_params = beta.fit(zone_df['kg'])\n",
    "    ms_mean, ms_std = zone_df['m/s'].mean(), zone_df['m/s'].std()\n",
    "\n",
    "    # Calculate number of observations for given number of years\n",
    "    total_hours = num_years * 365.25 * 24\n",
    "    num_observations = int(total_hours / timediv_mean)\n",
    "    \n",
    "    # Set the starting datetime to January 1st, 2000, 00:00:00\n",
    "    current_datetime = datetime(2000, 1, 1, 0, 0, 0)\n",
    "\n",
    "    # Initialize the new dataframe and generate the data\n",
    "    simulated_df = pd.DataFrame(index=range(num_observations))\n",
    "    simulated_df['timediv h'] = gamma(*timediv_params).rvs(size=num_observations).round(0)\n",
    "    simulated_df['datetime'] = simulated_df['timediv h'].cumsum().apply(lambda x: current_datetime + timedelta(hours=x))\n",
    "    simulated_df['kg'] = beta(*kg_params).rvs(size=num_observations).round(0)\n",
    "    simulated_df['m/s'] = norm(ms_mean, ms_std).rvs(size=num_observations).round(0)\n",
    "    simulated_df['kj'] = 0.5 * simulated_df['kg'] * (simulated_df['m/s']**2) /1000\n",
    "    \n",
    "    return simulated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulated dataframes\n",
    "simulated1 = generate_simulated_data(zone1)\n",
    "simulated1['zone'] = 1\n",
    "simulated2 = generate_simulated_data(zone2)\n",
    "simulated2['zone'] = 2\n",
    "simulated1.describe()\n",
    "# m/s is the min 0 which is not possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([zone1, simulated1], axis=1, keys=['zone1', 'simulated1']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histograms for 'kg', 'm/s', and 'timediv h'\n",
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(10, 10))\n",
    "\n",
    "# Histogram for 'kg'\n",
    "ax1.hist(zone1['kg'], bins=20, color='red', alpha=0.5, label='Zone 1', density=True)\n",
    "ax1.hist(simulated1['kg'], bins=20, color='orange', alpha=0.5, label='Simulated 1', density=True)\n",
    "ax2.hist(zone2['kg'], bins=20, color='blue', alpha=0.5, label='Zone 2', density=True)\n",
    "ax2.hist(simulated2['kg'], bins=20, color='green', alpha=0.5, label='Simulated 2', density=True)\n",
    "\n",
    "ax1.set_xlabel('kg')\n",
    "ax1.set_ylabel('Frequency Density')\n",
    "ax1.legend()\n",
    "ax2.set_xlabel('kg')\n",
    "ax2.legend()\n",
    "\n",
    "# Histogram for 'm/s'\n",
    "ax3.hist(zone1['m/s'], bins=20, color='red', alpha=0.5, label='Zone 1', density=True)\n",
    "ax3.hist(simulated1['m/s'], bins=20, color='orange', alpha=0.5, label='Simulated 1', density=True)\n",
    "ax4.hist(zone2['m/s'], bins=20, color='blue', alpha=0.5, label='Zone 2', density=True)\n",
    "ax4.hist(simulated2['m/s'], bins=20, color='green', alpha=0.5, label='Simulated 2', density=True)\n",
    "\n",
    "ax3.set_xlabel('m/s')\n",
    "ax3.set_ylabel('Frequency Density')\n",
    "ax3.legend()\n",
    "ax4.set_xlabel('m/s')\n",
    "ax4.legend()\n",
    "\n",
    "# Histogram for 'timediv h'\n",
    "ax5.hist(zone1['timediv h'], bins=20, color='red', alpha=0.5, label='Zone 1', density=True)\n",
    "ax5.hist(simulated1['timediv h'], bins=20, color='orange', alpha=0.5, label='Simulated 1', density=True)\n",
    "ax6.hist(zone2['timediv h'], bins=20, color='blue', alpha=0.5, label='Zone 2', density=True)\n",
    "ax6.hist(simulated2['timediv h'], bins=20, color='green', alpha=0.5, label='Simulated 2', density=True)\n",
    "\n",
    "ax5.set_xlabel('timediv h')\n",
    "ax5.set_ylabel('Frequency Density')\n",
    "ax5.legend()\n",
    "ax6.set_xlabel('timediv h')\n",
    "ax6.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the latest end datetime of the two dataframes\n",
    "max_datetime = min(simulated1['datetime'].max(), simulated2['datetime'].max())\n",
    "\n",
    "# Set the end datetime of both dataframes to be the same\n",
    "simulated1 = simulated1[simulated1['datetime'] <= max_datetime]\n",
    "simulated2 = simulated2[simulated2['datetime'] <= max_datetime]\n",
    "\n",
    "# Merge the two dataframes together, sort by datetime, and reset the index\n",
    "simulated_df = pd.concat([simulated1, simulated2])\n",
    "simulated_df = simulated_df.sort_values('datetime')\n",
    "simulated_df = simulated_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_df.tail(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because the reaction time is 24h we will asume that the nets will get emptied every evening if there are stones in it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column that calculates the cumulative kg already in the net.\n",
    "\n",
    "# first group the data by date\n",
    "grouped_df = simulated_df.groupby(simulated_df['datetime'].dt.date)\n",
    "\n",
    "# then calculate the cumulative sum of 'kg' within each group \n",
    "simulated_df['cumulative_kg'] = grouped_df['kg'].cumsum()\n",
    "# and subtract the 'kg' valueof the new stone to get the weight in the net\n",
    "simulated_df['cumulative_kg'] =  simulated_df['cumulative_kg'] - simulated_df['kg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should we disregard the rest stones of the day if the net broke trough?\n",
    "# after this the road probably gets closed\n",
    "\n",
    "# Add a new column 'breakthrough'\n",
    "simulated_df['breakthrough'] = 0\n",
    "\n",
    "# Set breakthrough to 1 where conditions are met\n",
    "condition1 = (simulated_df['kj'] > 1000)\n",
    "condition2 = (simulated_df['cumulative_kg'] > 2000) & (simulated_df['kj'] > 500)\n",
    "simulated_df.loc[condition1 | condition2, 'breakthrough'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i look at the tail to make shure the cumulative_kg got calculated correctly.\n",
    "\n",
    "simulated_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability of a breakthrough\n",
    "first_day = simulated_df['datetime'].min().date()\n",
    "last_day = simulated_df['datetime'].max().date()\n",
    "num_days = (last_day - first_day).days + 1\n",
    "\n",
    "breaktroughs_prbability = (simulated_df['breakthrough'] == 1).sum() / num_days\n",
    "breaktroughs_prbability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_df['breakthrough'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 5m car driving 60 will be in this zone for: \n",
    "velocity = 60 / 3.6\n",
    "print('velocity:', velocity, 'm/s')\n",
    "danger_time = 5 / velocity\n",
    "print('danger time: ', danger_time, 's')\n",
    "# with 1200 cars a day this will be that amount of seconds in danger:\n",
    "total_danger_time = 1200 * danger_time\n",
    "print('total danger time:', total_danger_time, 's')\n",
    "# precentage of cars being in danger per day:\n",
    "danger_time_proportion = total_danger_time / (24 * 60 * 60)\n",
    "print('danger time proportion: ', danger_time_proportion * 100, '%')\n",
    "\n",
    "# how likely is it that a car will be in danger and the net will break trough?\n",
    "dead_probability = breaktroughs_prbability * danger_time_proportion\n",
    "print('dead probability:', dead_probability * 100, '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steinschlag-t3PavukW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
